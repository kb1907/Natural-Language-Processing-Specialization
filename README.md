# Natural Language Processing Specialization
Natural Language Processing Specialization - Projects- Notes

-----------------------------------------------------------------------------------

<img src="https://aikademi.com/wp-content/uploads/2018/01/deeplearning.png" width="300" height="300">

## WHAT  I LEARNED
--------------------------------------

- Use logistic regression, naÃ¯ve Bayes, and word vectors to implement sentiment analysis, complete analogies & translate words.

- Use dynamic programming, hidden Markov models, and word embeddings to implement autocorrect, autocomplete & identify part-of-speech tags for words.

- Use recurrent neural networks, LSTMs, GRUs & Siamese networks in Trax for sentiment analysis, text generation & named entity recognition.

- Use encoder-decoder, causal, & self-attention to machine translate complete sentences, summarize text, build chatbots & question-answering.



### Reference
--------------------------

- In this folder, [Natural Language Processing Specialization](https://www.coursera.org/specializations/natural-language-processing) projects and notes can be found.

- **DeepLearning.AI** makes these slides available for educational purposes. 

- All the best ðŸ¤˜


## There are 4 Courses in this Specialization
--------------------------------------------------

## [Course 1 -  Natural Language Processing with Classification and Vector Spaces](https://github.com/kb1907/Natural-Language-Processing-Specialization/tree/main/Natural%20Language%20Processing%20with%20Classification%20and%20Vector%20Spaces)

- [In the first course of the Natural Language Processing Specialization](https://www.coursera.org/learn/classification-vector-spaces-in-nlp?specialization=natural-language-processing)

- I performed sentiment analysis of tweets using logistic regression and then naÃ¯ve Bayes, 
- I used vector space models to discover relationships between words and used PCA to reduce the dimensionality of the vector space and visualize those relationships, and
- I wrote a simple English to French translation algorithm using pre-computed word embeddings and locality-sensitive hashing to relate words via approximate k-nearest neighbor search.  

**Projects**
--------------
- [Sentiment Analysis of Tweets using Logistic Regression](https://github.com/kb1907/Natural-Language-Processing-Specialization/blob/main/Natural%20Language%20Processing%20with%20Classification%20and%20Vector%20Spaces/Week1/Programming_Assignment_Logistic-Regression.ipynb)
- [Sentiment Analysis of Tweets using NaÃ¯ve Bayes](https://github.com/kb1907/Natural-Language-Processing-Specialization/blob/main/Natural%20Language%20Processing%20with%20Classification%20and%20Vector%20Spaces/Week2/Programming%20Assignment_Naive%20Bayes.ipynb)
- [Vector Space Models](https://github.com/kb1907/Natural-Language-Processing-Specialization/blob/main/Natural%20Language%20Processing%20with%20Classification%20and%20Vector%20Spaces/Week3/Programming%20Assignment_Vector%20Space%20Models.ipynb)
- [Naive Machine Translation and LSH](https://github.com/kb1907/Natural-Language-Processing-Specialization/blob/main/Natural%20Language%20Processing%20with%20Classification%20and%20Vector%20Spaces/Week4/Programming%20Assignment_Word%20Translation.ipynb)

------------------------------------------------

## [Course 2 -  Natural Language Processing with Probabilistic Models](https://github.com/kb1907/Natural-Language-Processing-Specialization/tree/main/Natural%20Language%20Processing%20with%20Probabilistic%20Models)


- [In the second course of the Natural Language Processing Specialization](https://www.coursera.org/learn/probabilistic-models-in-nlp?specialization=natural-language-processing)

- I wrote a simple auto-correct algorithm using minimum edit distance and dynamic programming,
- I applied the Viterbi Algorithm for part-of-speech (POS) tagging, which is vital for computational linguistics,
- I wrote a better auto-complete algorithm using an N-gram language model, and 
- I wrote my own Word2Vec model that uses a neural network to compute word embeddings using a continuous bag-of-words model.

**Projects**
--------------
- [Auto-correct algorithm using minimum edit distance and dynamic programming](https://github.com/kb1907/Natural-Language-Processing-Specialization/blob/main/Natural%20Language%20Processing%20with%20Probabilistic%20Models/Week1/Programming%20Assignment_Autocorrect.ipynb)
- [Parts-of-Speech Tagging (POS)](https://github.com/kb1907/Natural-Language-Processing-Specialization/blob/main/Natural%20Language%20Processing%20with%20Probabilistic%20Models/Week2/Programming%20Assignment_Part%20of%20Speech%20Tagging.ipynb)
- [Auto-complete algorithm using an N-gram language model](https://github.com/kb1907/Natural-Language-Processing-Specialization/blob/main/Natural%20Language%20Processing%20with%20Probabilistic%20Models/Week3/Programming%20Assignment_Autocomplete.ipynb)
- [Word Embeddings- Continuous Bag of Words (CBOW) model](https://github.com/kb1907/Natural-Language-Processing-Specialization/blob/main/Natural%20Language%20Processing%20with%20Probabilistic%20Models/Week4/Programming%20Assignment_Word%20Embeddings.ipynb)


-------------------------------------------------

## [Course 3 - Natural Language Processing with Sequence Models](https://github.com/kb1907/Natural-Language-Processing-Specialization/tree/main/Natural%20Language%20Processing%20with%20Sequence%20Models)


- [In the third course of the Natural Language Processing Specialization](https://www.coursera.org/learn/sequence-models-in-nlp?specialization=natural-language-processing)

- I trained a neural network with GLoVe word embeddings to perform sentiment analysis of tweets,
- I generated synthetic Shakespeare text using a Gated Recurrent Unit (GRU) language model,
- I trained a recurrent neural network to perform named entity recognition (NER) using LSTMs with linear layers, and 
- I used so-called â€˜Siameseâ€™ LSTM models to compare questions in a corpus and identify those that are worded differently but have the same meaning.

**Projects**
--------------
- [Sentiment Analysis with Deep Neural Networks](https://github.com/kb1907/Natural-Language-Processing-Specialization/blob/main/Natural%20Language%20Processing%20with%20Sequence%20Models/Week1/Programming%20Assignment_Sentiment%20with%20Deep%20Neural%20Networks.ipynb)
- [Synthetic Shakespeare text using a Gated Recurrent Unit (GRU) language model](https://github.com/kb1907/Natural-Language-Processing-Specialization/blob/main/Natural%20Language%20Processing%20with%20Sequence%20Models/Week2/Programming%20Assignment_Deep%20N-grams.ipynb)
- [Named Entity Recognition (NER) using LSTM](https://github.com/kb1907/Natural-Language-Processing-Specialization/blob/main/Natural%20Language%20Processing%20with%20Sequence%20Models/Week3/Programming%20Assignment_Named%20Entity%20Recognition%20(NER).ipynb)
- [Question duplicates using Siamese LSTM models](https://github.com/kb1907/Natural-Language-Processing-Specialization/blob/main/Natural%20Language%20Processing%20with%20Sequence%20Models/Week4/Programming%20Assignment_Question%20Duplicates.ipynb)



---------------------------------------------------


## [Course 4 - Natural Language Processing with Attention Models](https://github.com/kb1907/Natural-Language-Processing-Specialization/tree/main/Natural%20Language%20Processing%20with%20Attention%20Models)


- [In the fourth course of the Natural Language Processing Specialization](https://www.coursera.org/learn/attention-models-in-nlp?specialization=natural-language-processing)

- I translated complete English sentences into German using an encoder-decoder attention model,
- I built a Transformer model to summarize text, 
- I used T5 and BERT models to perform question-answering, and
- I built a chatbot using a Reformer model. 


**Projects**
--------------
- [Neural Machine Translation using an Encoder-Decoder Attention Model](https://github.com/kb1907/Natural-Language-Processing-Specialization/blob/main/Natural%20Language%20Processing%20with%20Attention%20Models/Week1/Programming%20Assignment_NMT%20with%20Attention.ipynb)
- [Transformer Summarizer](https://github.com/kb1907/Natural-Language-Processing-Specialization/blob/main/Natural%20Language%20Processing%20with%20Attention%20Models/Week2/Programming%20Assignment_Transformer%20Summarizer.ipynb)
- [Question Answering using Text to Text Transfer from Transformers and BERT models](https://github.com/kb1907/Natural-Language-Processing-Specialization/blob/main/Natural%20Language%20Processing%20with%20Attention%20Models/Week3/Programming%20Assignment_Question%20Answering.ipynb)
- [Chatbot](https://github.com/kb1907/Natural-Language-Processing-Specialization/blob/main/Natural%20Language%20Processing%20with%20Attention%20Models/Week4/Programming%20Assignment_Chatbot.ipynb)

------------------------------------------------


All the best ðŸ¤˜


