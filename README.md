# Natural Language Processing Specialization
Natural Language Processing Specialization - Projects- Notes

-----------------------------------------------------------------------------------

<img src="https://aikademi.com/wp-content/uploads/2018/01/deeplearning.png" width="300" height="300">

## WHAT  I LEARNED
--------------------------------------

- Use logistic regression, na√Øve Bayes, and word vectors to implement sentiment analysis, complete analogies & translate words.

- Use dynamic programming, hidden Markov models, and word embeddings to implement autocorrect, autocomplete & identify part-of-speech tags for words.

- Use recurrent neural networks, LSTMs, GRUs & Siamese networks in Trax for sentiment analysis, text generation & named entity recognition.

- Use encoder-decoder, causal, & self-attention to machine translate complete sentences, summarize text, build chatbots & question-answering.



### Reference
--------------------------

- In this folder, [Natural Language Processing Specialization](https://www.coursera.org/specializations/natural-language-processing) projects and notes can be found.

- **DeepLearning.AI** makes these slides available for educational purposes. 

- All the best ü§ò


## There are 4 Courses in this Specialization
--------------------------------------------------

## [Course 1 Natural Language Processing with Classification and Vector Spaces](https://github.com/kb1907/Natural-Language-Processing-Specialization/tree/main/Natural%20Language%20Processing%20with%20Classification%20and%20Vector%20Spaces)

- [In the first course of the Natural Language Processing Specialization](https://www.coursera.org/learn/classification-vector-spaces-in-nlp?specialization=natural-language-processing)

- I performed sentiment analysis of tweets using logistic regression and then na√Øve Bayes, 
- I used vector space models to discover relationships between words and used PCA to reduce the dimensionality of the vector space and visualize those relationships, and
- I wrote a simple English to French translation algorithm using pre-computed word embeddings and locality-sensitive hashing to relate words via approximate k-nearest neighbor search.  

**Projects**
--------------
- [Sentiment analysis of tweets using Logistic Regression](https://github.com/kb1907/Natural-Language-Processing-Specialization/blob/main/Natural%20Language%20Processing%20with%20Classification%20and%20Vector%20Spaces/Week1/Programming_Assignment_Logistic-Regression.ipynb)
- [Sentiment analysis of tweets using Na√Øve Bayes](https://github.com/kb1907/Natural-Language-Processing-Specialization/blob/main/Natural%20Language%20Processing%20with%20Classification%20and%20Vector%20Spaces/Week2/Programming%20Assignment_Naive%20Bayes.ipynb)
- [Vector Space Models](https://github.com/kb1907/Natural-Language-Processing-Specialization/blob/main/Natural%20Language%20Processing%20with%20Classification%20and%20Vector%20Spaces/Week3/Programming%20Assignment_Vector%20Space%20Models.ipynb)
- [Naive Machine Translation and LSH](https://github.com/kb1907/Natural-Language-Processing-Specialization/blob/main/Natural%20Language%20Processing%20with%20Classification%20and%20Vector%20Spaces/Week4/Programming%20Assignment_Word%20Translation.ipynb)


## [Course 2 Natural Language Processing with Probabilistic Models](https://github.com/kb1907/Natural-Language-Processing-Specialization/tree/main/Natural%20Language%20Processing%20with%20Probabilistic%20Models)


- [In the second course of the Natural Language Processing Specialization](https://www.coursera.org/learn/probabilistic-models-in-nlp?specialization=natural-language-processing)

- I wrote a simple auto-correct algorithm using minimum edit distance and dynamic programming,
- I applied the Viterbi Algorithm for part-of-speech (POS) tagging, which is vital for computational linguistics,
- I wrote a better auto-complete algorithm using an N-gram language model, and 
- I wrote my own Word2Vec model that uses a neural network to compute word embeddings using a continuous bag-of-words model.

**Projects**
--------------
- [Auto-correct algorithm using minimum edit distance and dynamic programming](https://github.com/kb1907/Natural-Language-Processing-Specialization/blob/main/Natural%20Language%20Processing%20with%20Probabilistic%20Models/Week1/Programming%20Assignment_Autocorrect.ipynb)
- [Parts-of-Speech Tagging (POS)](https://github.com/kb1907/Natural-Language-Processing-Specialization/blob/main/Natural%20Language%20Processing%20with%20Probabilistic%20Models/Week2/Programming%20Assignment_Part%20of%20Speech%20Tagging.ipynb)
- [Auto-complete algorithm using an N-gram language model](https://github.com/kb1907/Natural-Language-Processing-Specialization/blob/main/Natural%20Language%20Processing%20with%20Probabilistic%20Models/Week3/Programming%20Assignment_Autocomplete.ipynb)
- [Word Embeddings- Continuous Bag of Words (CBOW) model](https://github.com/kb1907/Natural-Language-Processing-Specialization/blob/main/Natural%20Language%20Processing%20with%20Probabilistic%20Models/Week4/Programming%20Assignment_Word%20Embeddings.ipynb)



## [Course 2 Natural Language Processing with Probabilistic Models](https://github.com/kb1907/Natural-Language-Processing-Specialization/tree/main/Natural%20Language%20Processing%20with%20Probabilistic%20Models)


- [In the second course of the Natural Language Processing Specialization](https://www.coursera.org/learn/probabilistic-models-in-nlp?specialization=natural-language-processing)

- I wrote a simple auto-correct algorithm using minimum edit distance and dynamic programming,
- I applied the Viterbi Algorithm for part-of-speech (POS) tagging, which is vital for computational linguistics,
- I wrote a better auto-complete algorithm using an N-gram language model, and 
- I wrote my own Word2Vec model that uses a neural network to compute word embeddings using a continuous bag-of-words model.

**Projects**
--------------
- [Auto-correct algorithm using minimum edit distance and dynamic programming](https://github.com/kb1907/Natural-Language-Processing-Specialization/blob/main/Natural%20Language%20Processing%20with%20Probabilistic%20Models/Week1/Programming%20Assignment_Autocorrect.ipynb)
- [Parts-of-Speech Tagging (POS)](https://github.com/kb1907/Natural-Language-Processing-Specialization/blob/main/Natural%20Language%20Processing%20with%20Probabilistic%20Models/Week2/Programming%20Assignment_Part%20of%20Speech%20Tagging.ipynb)
- [Auto-complete algorithm using an N-gram language model](https://github.com/kb1907/Natural-Language-Processing-Specialization/blob/main/Natural%20Language%20Processing%20with%20Probabilistic%20Models/Week3/Programming%20Assignment_Autocomplete.ipynb)
- [Word Embeddings- Continuous Bag of Words (CBOW) model](https://github.com/kb1907/Natural-Language-Processing-Specialization/blob/main/Natural%20Language%20Processing%20with%20Probabilistic%20Models/Week4/Programming%20Assignment_Word%20Embeddings.ipynb)







